---
title: "Introduction to Hierarchical Clustering"
---

Clustering tries to find structure in data by creating groupings of data with similar characteristics. The most famous clustering algorithm is likely K-means. Hierarchical clustering is an alternative class of algorithms that produce 1 to n clusters, where n is the number of observations in the data set. As you go down the hierarchy from 1 cluster (contains all the data) to n clusters (each observation is its own cluster), the clusters always become more and more similar (unless centroid linkage is used). There are two types of hierarchical clustering: agglomerative (bottom-up) and divisive (top-down).

Divisive hierarchical clustering works by starting with 1 cluster containing the entire data set. The observation with the highest average dissimilarity (farthest from the cluster by some metric) is reassigned to its own cluster. Any observations in the old cluster closer to the new cluster are assigned to the new cluster. This process repeats with the largest cluster until each observation is its own cluster.

Agglomerative clustering starts with each observation as its own cluster. The two closest clusters are joined into one cluster. The next closest clusters are grouped together and this process continues until there is only one cluster containing the entire data set.

*What does it mean to be close?*

In the section above, I neglected to define what "close" means. There are a variety of possible metrics, but I will list the 4 most popular: single-linkage, complete-linkage, average-linkage, and centroid-linkage.

**Single-Linkage**

Single-linkage (nearest neighbor) is the shortest distance between two observations in two clusters. It can sometimes produce clusters where observations in different clusters are closer together than observations at the opposite ends of their own clusters. These clusters can appear spread-out.

**Complete-Linkage**

Complete-linkage (farthest neighbor) is where distance is measured between the farthest observations in two clusters. This method usually produces tighter clusters than single-linkage, but these tight clusters can end up very close together. Along with average-linkage, it is one of the more popular distance metrics.

**Average-Linkage**

Average-linkage is where the distance between each pair of observations in each cluster are added up and divided by the number of pairs to get an average inter-cluster distance. Average-linkage and complete-linkage are the two most popular distance metrics in hiearchical clustering.

**Centroid-Linkage**

Centroid-linkage is the distance between the centroids of two clusters. As the centroids move with new observations, it is possible that the clusters are more similar to the new cluster than they were their individual clusters causing an inversion in the dendrogram. This problem doesn't arise in the other linkage methods because the clusters being merged will always be more similar to themselves than to the new cluster.

**Using Hierarchical Clustering on State-level Demographic Data in R**

The conception of regions is strong in how we categorize states in the United States. Regions are clusters of states defined by geography, but geography leads to additional economic, demographic, and cultural similarities between states. For example, Evangelical Christianity emerged and is concentrated in the Southeast. Southern Florida is very close to Cuba and is thus the concentration of Cuban refugees in the US. To study how similar states are to each other today (actually with 2017 data), I downloaded data containing info from the 2017 American Community Survey and used hierarchical clustering to group them. My dataset has many variables, so I used " eigenvector decomposition, a concept from quantum mechanics to tease apart the overlapping 'notes' in" demographic data (had to join the dog-pile to show everyone that I took a linear algebra class too).

# **I. Creating the State and County Level Data Sets**

It would be nice if data sets were ready for exploration and modeling as soon as they're downloaded, but unfortunately they often need to be reformatted and cleaned before the fun can begin. This is what I do in this section.

## **A. Getting the Demographic Data from the 2015 American Community Survey**

First, the 2017 American Community Survey needs to be loaded in. There are 74,001 rows (Census Tracts) and 37 columns for variables on demographic, economic, and other types of data.

```{r}
# loads the dplyr library for data manipulation
library(dplyr)
# reads in the 2015 American Community Survey .csv file
acs = read.csv('acs2017_census_tract_data.csv')

# prints out the dimensions of the 2015 ACS and it shows there are 74001 rows and 37 columns
dim(acs)

# prints out the column names of the acs
names(acs)

# prints out the first 5 entries showing that lots of the variables are in percentages and need
# to be converted to absolute numbers before aggregating up to the county and state level
head(acs, n = 3)
```

After checking for missing values, I found there are numerous missing values that need to be filled in.

```{r}
acs = acs %>%
  select(-c(IncomeErr, IncomePerCapErr))

# creates a single variable for the State and County names for easy grouping since County names
# can repeat in multiple states
acs$StateCounty = with(acs, paste0(State, '_', County))

# prints the number of missing values in each column
for (var in colnames(acs)){
    if (sum(is.na(acs[[var]])) > 0) print(paste(var, sum(is.na(acs[[var]]))))
}
```

It appeared there are some Census Tracts with no data at all, which was especially a problem in Puerto Rico. Other missing observations seemed to be in rows with plenty of other information.

```{r}
# creates a dataframe of the rows with missing values in any column
missing_data <- acs[rowSums(is.na(acs)) > 0,]
# prints out the first and last 6 rows with missing values to get an idea of the missing values
head(missing_data)
tail(missing_data)
```

Since Puerto Rico lacks electoral votes I excluded it from my analysis, dropping some missing observations. I also dropped Census Tracts where the population is 0 because they appear to have only empty columns. There were still missing observations to be dealt with after dropping the empty rows, so I chose to impute them.

```{r}
# removes Puerto Rico and Census Tracts with empty total populations from acs in a new
# dataframe
fill_acs = acs[!(acs$State == 'Puerto Rico' | acs$TotalPop == 0),]
# drops the CensusTract because it isn't important to the analysis
fill_acs = fill_acs %>%
    select(-TractId)
# prints the number of missing values in each column
for (var in colnames(fill_acs)){
    if (sum(is.na(fill_acs[[var]])) > 0) print(paste(var, sum(is.na(fill_acs[[var]]))))
}
```

I decided to impute missing observations with the mean value in the column within the same county, since there are multiple Census Tracts within a county. This is under the assumption that areas within a county are more similar to each other than the state or total US level. However, after this process, there is still 1 missing value.

```{r}
# this loops through the numeric columns in fill_acs and then loops through the rows with
# missing values and then imputes the missing values with the mean for the subset of rows in
# in the same StateCounty subset
for (i in which(sapply(fill_acs, is.numeric))) {
    for (j in which(is.na(fill_acs[, i]))) {
        # sets the missing value at row j and in column i to the mean of the non-missing values
        # in column i that are in the same county in the same state
        fill_acs[j, i] = mean(
            fill_acs[fill_acs[, 'StateCounty'] ==  fill_acs[j, 'StateCounty'], i], na.rm = TRUE)
    }
}

# prints the number of missing values in each column
for (var in colnames(fill_acs)){
    if (sum(is.na(fill_acs[[var]])) > 0) print(paste(var, sum(is.na(fill_acs[[var]]))))
}
```

I used the state average for the last missing value.

Some of the variables were in percentage terms, which would have made aggregating up from the Census Tract level difficult. To fix this, I translated the percentages to proportions, and multiplied them by the total population of the Census Tract, then rounded to the nearest integer (you can't have fractions of a person). I got a total income for the Census Tracts by multiplying the per capita income by the total population of the Census Tract and did the same for the commute time variable. This allowed all the variables to be summed up giving the total number of persons in a given demographic, professional, etc. group and the total income and commute time at the state and county levels. These variables were then divided by the population at the state or county level, so they became proportions, income per capita, and average commute time per person at the state and county levels.

```{r}
# does the same thing as the other loop but it sets the missing values to the state average
# because these entries with missing values likely have no other observations in the same
# county
for (i in which(sapply(fill_acs, is.numeric))) {
    for (j in which(is.na(fill_acs[, i]))) {
        fill_acs[j, i] = mean(fill_acs[fill_acs[, 'State'] ==  fill_acs[j, 'State'], i],  
                              na.rm = TRUE)
    }
}

# prints the number of missing values in each column if there are any
for (var in colnames(fill_acs)){
    if (sum(is.na(fill_acs[[var]])) > 0) print(paste(var, sum(is.na(fill_acs[[var]]))))
}

# creates a list containing the soon to be redundant income variables and which will be
# appended with variables to drop
vars_to_drop = c('Income', 'IncomeErr', 'IncomePerCap', 'IncomePerCapErr');

# this loop goes through variables that are in percentage terms and then turns them to absolute
# numbers for later aggregation up to the county and state level
for (var in c('Hispanic', 'White', 'Black', 'Native', 'Asian', 'Pacific', 'Poverty',
       'ChildPoverty', 'Professional', 'Service', 'Office', 'Construction',
       'Production', 'Drive', 'Carpool', 'Transit', 'Walk', 'OtherTransp',
       'WorkAtHome', 'PrivateWork', 'PublicWork', 'SelfEmployed', 'FamilyWork', 
        'Unemployment')){
    # creates a new column from the old variable that turns the percentage into a proportion
    # then multiplies by the number of people in the census tract and then rounds that to the
    # nearest whole number because you cannot have fractions of a person
    fill_acs[[paste0(var,'_')]] = round((fill_acs[[var]] / 100) * fill_acs$TotalPop)
    # adds the old variable to the list to drop after the new variables are created
    vars_to_drop = c(vars_to_drop, var)    
} 

# creates a variable that when the state and county level aggregation occurs and each 
# variable is divided by TotalPop will provide per capita income
fill_acs$income_weighted = fill_acs$IncomePerCap * fill_acs$TotalPop

# creates a variable that when the state and county level aggregation occurs and each 
# variable is divided by TotalPop will provide per capita income
fill_acs$MeanCommute = fill_acs$MeanCommute * fill_acs$TotalPop

# drops the redundant variables
fill_acs = fill_acs[ , !(names(fill_acs) %in% vars_to_drop)]

head(fill_acs)
```


```{r}
# creates a variable that has the state level data by summing each variable at that level
X_state = fill_acs %>%
    select(-County) %>%
        select(-StateCounty) %>%
            group_by(State) %>%
                summarise_each(funs(sum))

# looks at the first and last 3 rows to check the appearance of the data
head(X_state, n = 3)
tail(X_state, n = 3)
# checks the dimensions (we expect 51 rows for 50 states and 1 for DC)
dim(X_state)
```


The variables in the state and county levels can be summarised to give some descriptive statistics, but there are 31 variables. Analyzing each of these individually is difficult and an entire project can be made of focusing in on a couple variables at a time. Dimensionality reduction can reduce this problem.

```{r}
# these two loops divide all numeric variables, excluding TotalPop, by TotalPop so they are now
# all now proportions of the total population, except for MeanCommute and income_weighted;
# MeanCommute is now the mean commute time at the new aggregated level and income_weighted
# is the income per person in the new aggregated level
for (i in 3:length(colnames(X_state))) {
    X_state[, i] = X_state[, i] / X_state$TotalPop
}

summary(X_state)
```

## **B.  Getting the 2016 Presidential Election Data**
### **i. Merging State Level Data with Voting Data**

Wrangling the state level electoral data was a much easier process. All that needed to be done is to filter the data set to only include the year 2016, then group the number of votes for each candidate in each state. I say candidate and not party because some candidates were on the ticket for multiple parties (like Secretary Clinton in NY who was on the Democratic and Working Families ticket). Then I created a binary variable for if Clinton won the state or not (TRUE if she did and FALSE if she did not) and attached this dataframe to the state level data from the American Community Survey.

```{r}
# reads in a .csv of presidential election data going back to 1976 
# (source: https://electionlab.mit.edu/data)
state_votes = read.csv('1976-2016-president.csv')

tail(state_votes, n = 10)
```


```{r}
# creates a percentage of votes won by the total votes for every row
state_votes$per_vote = state_votes$candidatevotes / state_votes$totalvotes

# creates a dataframe filtering out all non-2016 elections, grouping by state and 
# candidate because some candidates were the nominee for multiple parties 
# (like Clinton in New York for the Democratic and Working Famalies Party)
all_candidates = state_votes %>%
        filter(year == 2016) %>%
            group_by(state, candidate) %>%
                summarise_at(c('per_vote'), sum)

# creates a dataframe holding Clinton's percentage of the total vote in each state and DC
clinton_votes = all_candidates %>%
    group_by(state) %>%
        filter(candidate == 'Clinton, Hillary') %>%
            mutate(clinton_per = per_vote) %>%
                select(state, clinton_per)

# creates a dataframe holding Clinton's percentage of the total vote in each state and DC                       
trump_votes = all_candidates %>%
    group_by(state) %>%
        filter(candidate == 'Trump, Donald J.') %>%
            mutate(trump_per = per_vote) %>%
                select(state, trump_per)

# joins the two dataframes so each row is a state with Clinton and Trump's vote shares
y_state = merge(x = clinton_votes, 
               y = trump_votes, 
               by = 'state', 
               all = F)

# renames the state column as State to make the join with the other state level data set
# easier
colnames(y_state)[1] <- 'State'

# creates a dummy variable for if Trump won the county
y_state$clinton_win = y_state$clinton_per > y_state$trump_per

# joins the two dataframes so each row is a state with Clinton and Trump's vote shares
state = merge(x = X_state, 
               y = y_state, 
               by = 'State', 
               all = F)

head(state)
```


# **II. Dimensionality Reduction and Hierarchical Clustering**

## **A. Initial Exploration and Removing Highly Correlated Features**

Some of the features in the ACS are linear combinations of each other (like Men and Women) or are likely to be highly correlated with each other (like Poverty and ChildPoverty). I removed these variables before proceeding further with analysis.

```{r}
# creates new dataframes for the independent and dependent variables now that some rows have
# been eliminated during the join process
X_state = state %>%
    select(-c('clinton_per', 'trump_per', 'clinton_win'))

y_state = state %>%
    select(c('clinton_per', 'trump_per', 'clinton_win'))
```

The ACS has too many features to produce a full correlogram, so I broke the variables up into 3 groups to visualize some of the correlations, although that does not allow for all possible correlations correlations to be seen.

In the first correlogram, it is obvious that the proportion of men and women have a strong inverse correlation. This makes sense because there is only 100% of people in an area. An increase of 1% in one sex means the percentage of the other sex must decrease by 1%. 

```{r}
plot(X_state[,2:13], col='blue')
```

In the next correlogram, it is obvious that Poverty and ChildPoverty are highly positively correlated. This also is intuitive because the children of parents in poverty will be in poverty as well.

```{r}
plot(X_state[,14:21], col='blue')
```

No super strong correlations pop out in the last correlogram, except possibly a negative correlation between the proportion of people in the private versus public sector. That does not mean that there are no other strong correlations in the data set, because the correlogram for the entire data set was too large to visualize.

```{r}
plot(X_state[,22:32], col='blue')
```

Using a function to identify highly correlated (|correlation| > 0.9) features for removal, identified 2 features we identified visually for removal (Men, Poverty), as well other redundant variables we did not identify. After removing the redundant variables and recalculating the correlations, Drive and OtherTransp were identified as redundant due to strong correlations with other variables, so they were removed as well.

```{r}
# imports caret to identify corrleated features
library(caret)

# finds correlated features with threshold 0.9
state_correlated_features = findCorrelation(cor(X_state[,-1]), cutoff = 0.9, exact = T)

while (is.null(colnames(X_state[,state_correlated_features])) == F) {
  # finds correlated features with threshold 0.9
  state_correlated_features = findCorrelation(cor(X_state[,-1]), cutoff = 0.9, exact = T)
  # prints the name of correlated features
  print(colnames(X_state[,state_correlated_features]))
  # removes the highly correlated variables
  X_state = X_state[,-state_correlated_features]
}
```


## **B. Principal Component Analysis**

Even after removing the highly correlated variables there are still too many variables for clustering; the curse of dimensionality rears its head. In 24-dimensional space similar data points can be very far apart. For that reason, I used Principal Components Analysis to reduce the number of dimensions for which clustering was performed on.

Performing Principal Component Analysis on the independent variables showed about 79.3% of the variance in the independent variables is explained by the first 5 principal components. The fifth principal component is also the last principal compenent that explains more than 5% of the variance in the independent variables. I used that as an arbitrary cutting off point because I didn't want to greatly exceed 4 dimensions for clustering.

```{r}
# performs principal component analysis on the the independent variables and scales them all to have mean 0 and standard deviation 1
pc.state.full = prcomp(X_state[, -1], scale = T)
# creates a variable that stores the percentage of variance explained by each principal component
pve.state.full = 100 * pc.state.full$sdev ^ 2 / sum(pc.state.full$sdev ^ 2)

summary(pc.state.full)
```

This Scree Plot visualizes the proportion of variance explained by each principal component. After the 5th one, the plot starts to bottom out in terms of the proportion of variance explained by the components.

```{r}
# produces a scree plot for the state level data
plot(pve.state.full, type='o', main='Scree Plot for the 2017 ACS data', 
     ylab='Percentage of Variance Explained', xlab='Principal Component', col='blue')
```

This cumulative proportion of variance explained plot just illustrates that the cumulative percentage of variance explained is about 80% around the 5th principal component.

```{r}
# plots the cumulative percentage of variance explained by the principal components 
plot(cumsum(pve.state.full), type='o', main='Cumulative Percentage of Variance Explained for the 2017 ACS data', 
     ylab='Cumulative % of Variance Explained', xlab='Principal Component', col='blue')
```

This plot shows the 1st and 2nd principal components plotted against each other with blue dots for Clinton wins and red dots for Trump wins. It looks like the observations are fairly well seperated by the first 2 principal components.

```{r}
# this is a function I got from the ISLR textbook to produce colors in plots for the target variable

# takes in a vector as an argument
Cols=function(vec){
  # uses the rainbow function to make a color for each unique value in the input vector and returns it
  cols = rainbow(length(unique(vec))) 
  return(cols[as.numeric(as.factor(vec))]) 
}


# plots the 2 most important principal components against each other and assigns colors to the data points based on whether they voted for Clinton (Blue) or Trump (red)
plot(pc.state.full$x[,1:2], col=Cols(y_state$clinton_win), main='First vs Second Principal Component', pch=19, xlab="Z1",ylab="Z2")
```

Next, I performed hierarchical clustering with complete linkage on the first 5 principal components. I group cluster's based on a subjective reading of the dendrogram. 

It is clear that DC and Hawaii are very different from the other states (I refer to DC as a state for convenience's sake). 

In the first multi-state cluster, Alaska joins the upper Great Plain's region, although it is clear it is the most different of these states. Trump won all states within this cluster in 2016. This cluster is disproportionately rural, and white with above average Native American/Inuit populations.

Next, you have 3 Trump states (Texas, Arizona, and Florida) and 3 Clinton states (New Mexico, California, and Nevada). Out of the 6 states, 5 are in the Southwest. All have large hispanic populations, but Florida has a larger proportion of its hispanic population from Venezuelans and Cubans. Hispanic voters in Texas and Florida have historically been split between the Republican and Democratic parties, but this is something to watch going forward.

In the next cluster, you have the South, sans North Carolina, Georgia, and Virginia. In addition to most of the South, you have a subcluster containing Michigan, Ohio, Indiana, and Missouri. It is interesting that these states are closer (at least when using the 5 principal components of this data set) to most of the South than other states in the larger Midwest region. All states in this cluster voted for Trump.

Next, you have 6 high per capita GDP states on the East Coast. Virginia seems to have left the South for this cluster and is most similar to Connecticut. Connecticut and New Jersey have a lot of commuters to NYC, Virginia and Maryland have a lot of commuters to DC, and Massachusetts has its own large metropolitan area with Boston.

Following that cluster is a group of states containing the three northern most states in New England, the Upper Midwest minus Michigan, as well as Iowa and Nebraska. the cluster forms two continuous territories. Minus the Twin Cities, this cluster is more white than the country overall and rural. The New England States all went for Clinton, while only Minnesota went for Clinton among the Midwestern states.

The second to last cluster is an interesting grouping of North Carolina and Georgia and then Pennsylvania, Illinois, Delaware, and Rhode Island. The first 3 voted for Trump in 2016 and the latter half voted for Clinton.

The final cluster stretches from Washington in the northwest to Oklahoma in the southeast. It represents the Pacific Northwest, Mountain West, and part of the Midwest. The Pacific Northwest and Colorado went for Clinton, while the others voted for Trump.

```{r}
hc.complete = hclust(dist(pc.state.full$x[,1:5]),method='complete')
plot(hc.complete, labels = X_state$State, main='Dendrogram of Regional Clusters using 2017 ACS Data (Agglomerative)', xlab='', sub='',cex=0.7)
```

The results using divisive hierarchical clustering are similar, but Georgia and North Carolina return to the South.

```{r}
library(cluster)

div.hc = diana(pc.state.full$x[,1:5], diss = inherits(pc.state.full$x[,1:5], "dist"), metric = "euclidean")

plot(div.hc, labels = X_state$State, , main='Dendrogram of Regional Clusters using 2017 ACS Data (Divisive)', xlab='')
```

# **III. Conclusions**

In terms of voting behavior, about half of the clusters were completely homogeneous in terms of the 2016 electoral outcomes for each state, but about half were mixed. This suggests the data set is missing important information related to voting behavior in the 2016 election. Two possible variables that might help are the distribution of age and educational attainment for the state. Despite some of the interesting clusters from this exercise, they are not the final word on how to group states. This data set does not contain important variables (like median age and educational attainment) that could change the makeup of the clusters. Also, the first 5 principal components explain just under 80% of the variation in the data set. This leaves 20% of variation on accounted for, which could be omitting important information that affecting the clustering analysis.